How would you design auto-scaling rules for microservices in Kubernetes?
I’ll give a practical, production-ready design you can apply to most Kubernetes microservice landscapes, with concrete examples and the tradeoffs you should expect.

Goals & principles (short)

Scale horizontally first for stateless services (add/remove pods).

Keep pods fast to start/stop (short startup, readiness probe) so autoscaling reacts quickly.

Use multiple signals (CPU, memory, request latency, queue length, custom metrics) rather than a single CPU number.

Separate pod autoscaling from node autoscaling (HPA/KEDA/VPA vs Cluster Autoscaler).

Protect stability with cooldowns, stabilization windows, limits, and safety objects (PDBs, priority classes).

Observe & test — run load tests and chaos experiments.

Components you’ll normally use

Horizontal Pod Autoscaler (HPA, v2) — scale pods by CPU/memory or custom metrics (Prometheus adapter / metrics API).

Vertical Pod Autoscaler (VPA) — recommend or adjust CPU/memory; avoid auto on production stateless services unless you accept pod evictions. Use recommendation mode for ops insight.

Cluster Autoscaler — scale nodes up/down based on unschedulable pods and resource requests. Configure node groups (autoscaling groups) and max/min sizes.

KEDA (Kubernetes Event Driven Autoscaling) — ideal for queue/event-driven workloads (RabbitMQ, Kafka, Azure Service Bus, SQS) and scaling to zero.

Custom/autonomous controllers or Knative — for concurrency-based or request-latency SLO-driven scaling.

Prometheus + adapter — expose custom metrics to HPA (via custom.metrics.k8s.io) for latency, RPS, queue length, etc.

Observability — Prometheus, Grafana, alerts, synthetic tests, and dashboards for autoscaling signals.

Designing autoscaling rules — step-by-step
1) Classify your services

Stateless request/HTTP microservices → HPA on CPU/requests-per-second/latency.

Queue-driven workers → KEDA; scale on queue length or lag.

Stateful services / DBs → usually scale vertically or sharded; careful with horizontal scaling.

Latency/SLO-critical → SLO-driven autoscaling: target latency or error budget instead of raw CPU.

2) Set resource requests and limits

Set realistic requests (affects scheduling & cluster-autoscaler). limits to prevent noisy neighbors.

Use requests based on p50/p95 from real traffic. Don’t leave requests=0.

3) Choose scaling metrics and targets

CPU: simple baseline. Target 50–70% CPU utilization per pod is common.

Memory: only if memory patterns are stable and relevant.

RPS / concurrency: target RPS per pod (e.g., 100 RPS/pod).

Latency: use request latency (p95) to scale when latency approaches target SLO.

Queue length / backlog: scale workers by queue depth or messages per second.

Custom: DB connections, error rates, external API throttling.

4) Compose autoscalers

HPA (horizontal) for real-time pod count changes.

VPA (vertical) in recommendation mode to adjust requests for better packing; optionally auto in non-critical dev clusters.

Cluster Autoscaler to add nodes when there are unschedulable pods; configure multiple node pools (e.g., general-purpose, burstable GPU).

For event-driven scaling, use KEDA ScaledObjects.

5) Safety & stabilization

HPA behavior policy: limit aggressive scale-ups and scale-downs (policies + stabilizationWindowSeconds).
Example: max scale up 4 pods within 60s, scale down no faster than 1 pod per 60s; stabilizationWindow for scaleDown 5m.

Use PodDisruptionBudgets (PDBs) to ensure minimum availability.

Use priorityClass and preemption rules for critical services.

Graceful shutdown: terminationGracePeriodSeconds, preStop hooks, readiness probe to remove from load balancer before termination.

6) Node group and cost strategy

Make autoscaling groups for different workloads (e.g., small cheap nodes for web, larger nodes for heavy compute).

Set node group limits, and optional reserved capacity / overprovisioning (e.g., a tiny always-on deployment that is benign to keep some spare capacity).

Use Spot/Preemptible VMs for non-critical workloads with fallback node groups.

7) Telemetry & testing

Monitor: pod count, CPU/memory, queue length, request latency (p50/p95/p99), error rates, scheduling failures, node autoscaler events.

Load test (k6, JMeter) and run chaos tests (simulate node loss, high latency).

Track cost per scaling decision.

Example manifests
HPA v2 (CPU + custom metric example)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: orders-svc-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orders-svc
  minReplicas: 3
  maxReplicas: 30
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60    # target 60% CPU
    - type: Pods
      pods:
        metric:
          name: custom_request_latency_ms
        target:
          type: AverageValue
          averageValue: "200"      # keep average request latency below 200ms
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Pods
          value: 10
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60


(Requires exposing custom_request_latency_ms via the custom metrics API — often via Prometheus Adapter.)

KEDA ScaledObject (RabbitMQ queue length)
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: email-worker-scaledobject
spec:
  scaleTargetRef:
    name: email-worker-deployment
  minReplicaCount: 0
  maxReplicaCount: 50
  triggers:
    - type: rabbitmq
      metadata:
        queueName: email-queue
        host: "amqp://user:pass@rabbitmq.svc.cluster.local:5672/"
        queueLength: "100"  # 1 pod per 100 messages

HPA tuning recommendations (practical numbers)

minReplicas: at least 2–3 for critical services to avoid single-point-of-failure and to warm caches.

target CPU: 50–70% (lower for latency-sensitive services).

RPS/Pod: derive from stress tests; e.g., if each pod handles 200 RPS at SLO, set HPA target to requests_per_pod = desired_rps / pods (expose as custom metric).

maxReplicas: set based on backend capacity (DB connections, external API rate limits) and cluster limits.

stabilizationWindow / cooldown: scaleDown stabilization ~3–5 minutes; scaleUp faster but bounded by policies.

SLO-driven autoscaling (advanced)

Convert SLO into a target (e.g., p95 latency < 200ms).

Expose a metric like observed_latency_ratio_to_target (e.g., latency / SLO).

HPA scales on that metric; or use a custom controller that computes desired replicas = ceil(current_load / RPS_per_pod) * safety_factor.

This ties autoscaling to user experience instead of raw resource utilization.

Common pitfalls & how to avoid them

No resource requests → Cluster Autoscaler can't predict capacity; cause scheduling storms. Always set requests.

Relying only on CPU → ignores I/O, external latency, queue depth. Use multi-metric HPAs.

Too aggressive scale-down → causes churn and cold starts; use stabilization windows.

VPA + HPA conflict → use VPA in recommendation mode or isolate vertical scaling to services where HPA doesn't apply.

Ignoring startup time → long startup times reduce autoscaler effectiveness; optimize image size, init times.

Not accounting for DB/third-party limits → maxReplicas must consider connection pools and rate limits.

Checklist before rollout

Set requests/limits for all pods.

Add liveness/readiness probes and preStop hook + terminationGracePeriodSeconds.

Define HPA with sensible min/max and behavior.

Expose custom metrics if needed (Prometheus adapter).

Configure Cluster Autoscaler with node pools; set safe min/max node counts.

Add PDBs and priority classes.

Test with load tests and observe scaling timeline.

Add alerts for scaling anomalies (failed scale-ups, high OOMs, long scheduling times).

Quick decision guide

If queue-based work → KEDA (scale to zero capability).

If HTTP RPS/concurrency → HPA on requests or custom metric (Prometheus).

If short-lived spikes → use burstable node pool + scaleUp policies and ensure fast pod startup.

If you need per-pod resource tuning → VPA recommendation + periodic updates.

If cost-sensitive and can tolerate cold starts → combine scale-to-zero (KEDA/Knative) for low-traffic services.
