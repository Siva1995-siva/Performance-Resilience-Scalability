One microservice adds 400ms latency randomly ‚Äî how do you troubleshoot?

a clear, practical, production-style troubleshooting guide for a microservice that randomly adds ~400 ms latency. This kind of intermittent delay is one of the hardest to debug ‚Äî but there are only a few root-cause categories. I‚Äôll give you step-by-step diagnostics, probable causes, and how to find them with real commands, plus Java/Spring code examples that often cause this issue.

‚úÖ Step 1 ‚Äî Confirm the pattern

Before jumping to logs, answer:

‚úî Is the 400 ms a consistent minimum delay, or a random spike?

Consistent 400 ms ‚Üí usually DNS, connection pool wait, TLS handshake.

Random 400 ms ‚Üí GC pauses, thread starvation, lock contention, slow downstream calls.

‚úî Does it affect ALL endpoints or only one?

All endpoints ‚Üí GC, threadpool exhaustion, DNS/TLS.

One endpoint ‚Üí blocking I/O, DB query, expensive operation.

‚úî Is it in your service or downstream?

Enable server & client trace IDs to see where the 400 ms occurs.

‚úÖ Step 2 ‚Äî Add high-cardinality trace logging (first move)

Enable timing in logs:

long start = System.currentTimeMillis();
// ‚Ä¶ actual logic
log.info("Latency={}ms traceId={}", System.currentTimeMillis() - start, traceId);


Then correlate with:

DB logs

Downstream logs

GC logs

Thread dumps

Access logs (Nginx/API Gateway)

This isolates which segment consumes the 400ms.

‚úÖ Step 3 ‚Äî Collect evidence
1) Thread dumps during a spike
kubectl exec -it <pod> -- jstack <pid> > /tmp/tdump.txt


Look for:

Threads stuck in WAITING on DB pool / HTTP client pool

Frequent parking under locks

Same stack repeatedly showing blocking call

Example sign of root cause:

at com.zaxxer.hikari.pool.HikariPool.getConnection(...)
WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer


‚Üí DB pool exhaustion ‚Üí 100‚Äì500ms waits

2) GC diagnostics

Enable GC logging if not already:

- Xlog:gc*:file=/logs/gc.log:time


If you see:

Pause Young (Normal) 350ms


‚Üí Minor GC pauses cause 300‚Äì500 ms latency spikes.

3) Connection pool wait-time metrics

Expose Hikari metrics:

hikaricp.connections.acquire
hikaricp.connections.active
hikaricp.connections.pending


If pending > 0 ‚Üí calls waiting up to your connectionTimeout (default 30s), but often they spike around 300‚Äì500 ms.

Typical misconfig example:

spring.datasource.hikari.maximum-pool-size: 10


But traffic requires 50+ concurrent threads ‚Üí 400 ms stalls.

4) Network & DNS

Run:

kubectl exec <pod> -- nslookup api.my-upstream.svc


Or trace DNS latency:

time curl -v http://upstream


If random DNS lookup takes 200‚Äì500 ms ‚Üí DNS cache misses or misconfigured search domains.

Common cause:

JVM default resolver + Kubernetes DNS slowness = 250‚Äì500 ms sometimes.

Fix:

-Dsun.net.inetaddr.ttl=60

5) Downstream slow calls

Enable client-side timing (WebClient or RestTemplate).

For WebClient:
exchange.filter((req, next) -> {
    long t = System.currentTimeMillis();
    return next.exchange(req)
        .doOnSuccess(r -> log.info("Upstream latency {}ms", System.currentTimeMillis() - t));
});


If upstream occasionally takes 350‚Äì450 ms ‚Üí issue is downstream.

‚úÖ Step 4 ‚Äî Top Real-World Causes of Random 400 ms Latency

Below are the most common, ranked by probability.

1) Connection Pool Waiting (DB or HTTP) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Most common)

Symptoms:

Latency spikes around 300‚Äì500 ms

Thread dump shows threads waiting in Hikari

Metrics show pending acquisition

Example real culprit:

maximumPoolSize=10


but your service receives 200 concurrent requests ‚Üí pool queue delay ‚âà 400ms.

Fix:

Increase pool size

Reduce request concurrency (bulkhead)

Use async/reactive where possible

2) Minor GC Pauses (G1 or Parallel) ‚≠ê‚≠ê‚≠ê‚≠ê

Minor GCs often take 100‚Äì500 ms.

Look for:

Pause Young (Mixed) 420ms

Fix:

Increase heap

Reduce allocation rate

Tune G1GC: -XX:MaxGCPauseMillis=200

3) DNS Resolution Delays ‚≠ê‚≠ê‚≠ê‚≠ê

Random DNS slowness causes 250‚Äì500 ms delays.

Typical in Kubernetes.

Fix:

Enable DNS caching:

-Dsun.net.inetaddr.ttl=60


Reduce search domains in k8s

Use a sidecar DNS cache (NodeLocal DNSCache)

4) Thread Pool Starvation ‚≠ê‚≠ê‚≠ê

If Tomcat/Netty threads are blocked, requests wait 300‚Äì700 ms before processing.

Run:

jstack | grep http-nio | wc -l


Fix:

server.tomcat.threads.max: 300

5) Slow downstream dependency ‚≠ê‚≠ê‚≠ê

Your upstream occasionally spikes by 400ms.

Check logs or tracing:

Upstream latency: 412ms


Fix:

Timeouts

Retries with jitter

Circuit breakers

Example timeout config:

webclient:
  connectTimeout: 200
  readTimeout: 300

6) Lock Contention in Your Code ‚≠ê‚≠ê‚≠ê

Java example that causes random 300‚Äì500ms pauses:

synchronized(cache) {
    cache.computeIfAbsent(...);  // heavy computation
}


Thread dumps show many threads BLOCKED on the same lock.

Fix:

Replace synchronized with ConcurrentHashMap

Avoid blocking inside locks

7) Cold Start / Lazy Init ‚≠ê‚≠ê

Some microservices lazily load data on first call, causing one-time 200‚Äì700 ms latency.

Fix:

Warm-up APIs on startup

Preload caches

8) Network retries or Nagle‚Äôs algorithm ‚≠ê‚≠ê

TCP-level jitter adds 200‚Äì400ms occasionally.

Fix:

System.setProperty("http.keepAlive", "true");

üéØ Step 5 ‚Äî Put it all together: 400 ms Latency Debug Checklist
Immediate

 Check thread-dump for pool waits

 Check GC logs for pauses

 Measure DNS resolution

 Check DB/HTTP pool pending threads

 Add request-level timing logs

 Look at traces (Jaeger/Zipkin/Tempo/OpenTelemetry)

If still unclear

Capture pcap on pod

Enable Java Flight Recorder (JFR)

Run chaos load to provoke spikes on demand
